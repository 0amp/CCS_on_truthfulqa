{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b6d95d0258414faa1444f06ccd3258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Couldn't build proto file into descriptor pool: duplicate file name (sentencepiece_model.proto)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdeberta\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     24\u001b[0m     model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/deberta-v2-xxlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[1;32m     26\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForMaskedLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/deberta-v2-xxlarge\u001b[39m\u001b[39m\"\u001b[39m, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[1;32m     27\u001b[0m     model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:553\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1786\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1788\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1789\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1790\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1791\u001b[0m     init_configuration,\n\u001b[1;32m   1792\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1793\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1794\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1795\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1796\u001b[0m )\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1923\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1923\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1924\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1925\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1926\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1927\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1928\u001b[0m     )\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:129\u001b[0m, in \u001b[0;36mDebertaV2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    116\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    128\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    130\u001b[0m         vocab_file,\n\u001b[1;32m    131\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    132\u001b[0m         do_lower_case\u001b[39m=\u001b[39;49mdo_lower_case,\n\u001b[1;32m    133\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[1;32m    134\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    135\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    136\u001b[0m         sep_token\u001b[39m=\u001b[39;49msep_token,\n\u001b[1;32m    137\u001b[0m         pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    138\u001b[0m         cls_token\u001b[39m=\u001b[39;49mcls_token,\n\u001b[1;32m    139\u001b[0m         mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    140\u001b[0m         split_by_punct\u001b[39m=\u001b[39;49msplit_by_punct,\n\u001b[1;32m    141\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:113\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m TokenizerFast\u001b[39m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    111\u001b[0m \u001b[39melif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1066\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1060\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn instance of tokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_name\u001b[39m}\u001b[39;00m\u001b[39m cannot be converted in a Fast tokenizer instance. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1061\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo converter was found. Currently available slow->fast convertors: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[1;32m   1064\u001b[0m converter_class \u001b[39m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m-> 1066\u001b[0m \u001b[39mreturn\u001b[39;00m converter_class(transformer_tokenizer)\u001b[39m.\u001b[39mconverted()\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:425\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    421\u001b[0m requires_backends(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprotobuf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    423\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 425\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m sentencepiece_model_pb2 \u001b[39mas\u001b[39;00m model_pb2\n\u001b[1;32m    427\u001b[0m m \u001b[39m=\u001b[39m model_pb2\u001b[39m.\u001b[39mModelProto()\n\u001b[1;32m    428\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_tokenizer\u001b[39m.\u001b[39mvocab_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/transformers/utils/sentencepiece_model_pb2.py:35\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 35\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39;49mFileDescriptor(\n\u001b[1;32m     36\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msentencepiece_model.proto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     37\u001b[0m     package\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msentencepiece\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m     syntax\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mproto2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m     serialized_pb\u001b[39m=\u001b[39;49m_b(\n\u001b[1;32m     40\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x19\u001b[39;49;00m\u001b[39msentencepiece_model.proto\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39msentencepiece\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\xf4\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39mTrainerSpec\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39minput\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39minput_format\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39mmodel_prefix\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x41\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mmodel_type\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m$.sentencepiece.TrainerSpec.ModelType:\u001b[39;49m\u001b[39m\\x07\u001b[39;49;00m\u001b[39mUNIGRAM\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mvocab_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x38\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39m\\x63\u001b[39;49;00m\u001b[39m\\x63\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mpt_language\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x15\u001b[39;49;00m\u001b[39mself_test_sample_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x63\u001b[39;49;00m\u001b[39mharacter_coverage\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m.9995\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x13\u001b[39;49;00m\u001b[39minput_sentence_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m$\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39mshuffle_input_sentence\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x13\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39mmining_sentence_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39mtraining_sentence_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39mseed_sentencepiece_size\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x07\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39mshrinking_factor\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m.75\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m!\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x13\u001b[39;49;00m\u001b[39mmax_sentence_length\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x34\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x39\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39mnum_threads\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x36\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1d\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39mnum_sub_iterations\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m$\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39mmax_sentencepiece_length\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x36\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m%\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39msplit_by_unicode_script\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x15\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1d\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39msplit_by_number\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m!\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x13\u001b[39;49;00m\u001b[39msplit_by_whitespace\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39mtreat_whitespace_as_suffix\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x66\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39mlse\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39m\\x63\u001b[39;49;00m\u001b[39montrol_symbols\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39muser_defined_symbols\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x1f\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39mhard_vocab_limit\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m! \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39muse_all_vocab\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m\\x66\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39mlse\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39munk_id\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m( \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m\\x62\u001b[39;49;00m\u001b[39mos_id\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m) \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x31\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x11\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mos_id\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m* \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mpad_id\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m+ \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m-1\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39munk_piece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m- \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m<unk>\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39mbos_piece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m. \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m<s>\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x17\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39meos_piece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m/ \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m</s>\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39mpad_piece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m<pad>\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39munk_surface\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m, \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\xe2\u001b[39;49;00m\u001b[39m\\x81\u001b[39;49;00m\u001b[39m\\x87\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m5\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\t\u001b[39;49;00m\u001b[39mModelType\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39mUNIGRAM\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39mPE\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mWORD\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x43\u001b[39;49;00m\u001b[39mHAR\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\\xd1\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39mNormalizerSpec\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mname\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1c\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x14\u001b[39;49;00m\u001b[39mprecompiled_charsmap\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x61\u001b[39;49;00m\u001b[39m\\x64\u001b[39;49;00m\u001b[39m\\x64\u001b[39;49;00m\u001b[39m_dummy_prefix\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m&\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x18\u001b[39;49;00m\u001b[39mremove_extra_whitespaces\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mscape_whitespaces\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtrue\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x1e\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x16\u001b[39;49;00m\u001b[39mnormalization_rule_tsv\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39mSelfTestData\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x33\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39msamples\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.sentencepiece.SelfTestData.Sample\u001b[39;49m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mSample\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39minput\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\x65\u001b[39;49;00m\u001b[39mxpected\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\\xba\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mModelProto\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x37\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mpieces\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\'\u001b[39;49;00m\u001b[39m.sentencepiece.ModelProto.SentencePiece\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x30\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39mtrainer_spec\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39m.sentencepiece.TrainerSpec\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x36\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0f\u001b[39;49;00m\u001b[39mnormalizer_spec\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x1d\u001b[39;49;00m\u001b[39m.sentencepiece.NormalizerSpec\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x33\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39mself_test_data\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m\\x1b\u001b[39;49;00m\u001b[39m.sentencepiece.SelfTestData\u001b[39;49m\u001b[39m\\x1a\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39mSentencePiece\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39mpiece\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39mscore\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mtype\u001b[39;49m\u001b[39m\\x18\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m\\x0e\u001b[39;49;00m\u001b[39m\\x32\u001b[39;49;00m\u001b[39m,.sentencepiece.ModelProto.SentencePiece.Type:\u001b[39;49m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mNORMAL\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mJ\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39mType\u001b[39;49m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mNORMAL\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39mUNKNOWN\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x0b\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x07\u001b[39;49;00m\u001b[39m\\x43\u001b[39;49;00m\u001b[39mONTROL\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x0c\u001b[39;49;00m\u001b[39mUSER_DEFINED\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x04\u001b[39;49;00m\u001b[39m\\x12\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\x06\u001b[39;49;00m\u001b[39mUNUSED\u001b[39;49m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x05\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m*\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\\x08\u001b[39;49;00m\u001b[39m\\xc8\u001b[39;49;00m\u001b[39m\\x01\u001b[39;49;00m\u001b[39m\\x10\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x80\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39m\\x42\u001b[39;49;00m\u001b[39m\\x02\u001b[39;49;00m\u001b[39mH\u001b[39;49m\u001b[39m\\x03\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     41\u001b[0m     ),\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m _sym_db\u001b[39m.\u001b[39mRegisterFileDescriptor(DESCRIPTOR)\n\u001b[1;32m     46\u001b[0m _TRAINERSPEC_MODELTYPE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     47\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece.TrainerSpec.ModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m1174\u001b[39m,\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/google/protobuf/descriptor.py:1028\u001b[0m, in \u001b[0;36mFileDescriptor.__new__\u001b[0;34m(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mPlease link in cpp generated lib for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (name))\n\u001b[1;32m   1027\u001b[0m \u001b[39melif\u001b[39;00m serialized_pb:\n\u001b[0;32m-> 1028\u001b[0m   \u001b[39mreturn\u001b[39;00m _message\u001b[39m.\u001b[39;49mdefault_pool\u001b[39m.\u001b[39;49mAddSerializedFile(serialized_pb)\n\u001b[1;32m   1029\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(FileDescriptor, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't build proto file into descriptor pool: duplicate file name (sentencepiece_model.proto)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Let's just try IMDB for simplicity\n",
    "data = load_dataset(\"imdb\")[\"test\"]\n",
    "\n",
    "# Here are a few different model options you can play around with:\n",
    "# model_name = \"deberta\"\n",
    "# model_name = \"gpt-j\"\n",
    "model_name = \"t5\"\n",
    "\n",
    "# if you want to cache the model weights somewhere, you can specify that here\n",
    "cache_dir = None\n",
    "\n",
    "if model_name == \"deberta\":\n",
    "    model_type = \"encoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"gpt-j\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"t5\":\n",
    "    model_type = \"encoder_decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-11b\", cache_dir=cache_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-11b\", cache_dir=cache_dir)\n",
    "    model.parallelize()  # T5 is big enough that we may need to run it on multiple GPUs\n",
    "else:\n",
    "    print(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_encoder_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    decoder_text_ids = tokenizer(\"\", return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, decoder_input_ids=decoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"encoder_hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_hidden_states(model, tokenizer, input_text, layer=-1, model_type=\"encoder\"):\n",
    "    fn = {\"encoder\": get_encoder_hidden_states, \"encoder_decoder\": get_encoder_decoder_hidden_states,\n",
    "          \"decoder\": get_decoder_hidden_states}[model_type]\n",
    "\n",
    "    return fn(model, tokenizer, input_text, layer=layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdb(text, label):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive), \n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "    \n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"The following movie review expresses a \" + [\"negative\", \"positive\"][label] + \" sentiment:\\n\" + text\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, model_type, n=100):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for _ in tqdm(range(n)):\n",
    "        # for simplicity, sample a random example until we find one that's a reasonable length\n",
    "        # (most examples should be a reasonable length, so this is just to make sure)\n",
    "        while True:\n",
    "            idx = np.random.randint(len(data))\n",
    "            text, true_label = data[idx][\"content\"], data[idx][\"label\"]\n",
    "            # the actual formatted input will be longer, so include a bit of a marign\n",
    "            if len(tokenizer(text)) < 400:  \n",
    "                break\n",
    "                \n",
    "        # get hidden states\n",
    "        neg_hs = get_hidden_states(model, tokenizer, format_imdb(text, 0), model_type=model_type)\n",
    "        pos_hs = get_hidden_states(model, tokenizer, format_imdb(text, 1), model_type=model_type)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, data, model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a simple 50/50 train split (the data is already randomized)\n",
    "n = len(y)\n",
    "neg_hs_train, neg_hs_test = neg_hs[:n//2], neg_hs[n//2:]\n",
    "pos_hs_train, pos_hs_test = pos_hs[:n//2], pos_hs[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "\n",
    "# for simplicity we can just take the difference between positive and negative hidden states\n",
    "# (concatenating also works fine)\n",
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train, y_train)\n",
    "print(\"Logistic regression accuracy: {}\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProbe(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d, 100)\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear1(x))\n",
    "        o = self.linear2(h)\n",
    "        return torch.sigmoid(o)\n",
    "\n",
    "class CCS(object):\n",
    "    def __init__(self, x0, x1, nepochs=1000, ntries=10, lr=1e-3, batch_size=-1, \n",
    "                 verbose=False, device=\"cuda\", linear=True, weight_decay=0.01, var_normalize=False):\n",
    "        # data\n",
    "        self.var_normalize = var_normalize\n",
    "        self.x0 = self.normalize(x0)\n",
    "        self.x1 = self.normalize(x1)\n",
    "        self.d = self.x0.shape[-1]\n",
    "\n",
    "        # training\n",
    "        self.nepochs = nepochs\n",
    "        self.ntries = ntries\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # probe\n",
    "        self.linear = linear\n",
    "        self.probe = self.initialize_probe()\n",
    "        self.best_probe = copy.deepcopy(self.probe)\n",
    "\n",
    "        \n",
    "    def initialize_probe(self):\n",
    "        if self.linear:\n",
    "            self.probe = nn.Linear(self.d, 1)\n",
    "        else:\n",
    "            self.probe = MLPProbe(self.d)\n",
    "        self.probe.to(self.device)    \n",
    "\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Mean-normalizes the data x (of shape (n, d))\n",
    "        If self.var_normalize, also divides by the standard deviation\n",
    "        \"\"\"\n",
    "        normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "        if self.var_normalize:\n",
    "            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "        return normalized_x\n",
    "\n",
    "        \n",
    "    def get_tensor_data(self):\n",
    "        \"\"\"\n",
    "        Returns x0, x1 as appropriate tensors (rather than np arrays)\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        return x0, x1\n",
    "    \n",
    "\n",
    "    def get_loss(self, p0, p1):\n",
    "        \"\"\"\n",
    "        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n",
    "        \"\"\"\n",
    "        informative_loss = (torch.min(p0, p1)**2).mean(0)\n",
    "        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n",
    "        return informative_loss + consistent_loss\n",
    "\n",
    "\n",
    "    def get_acc(self, x0_test, x1_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes accuracy for the current parameters on the given test inputs\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "        acc = (predictions == y_test).mean()\n",
    "        acc = max(acc, 1 - acc)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Does a single training run of nepochs epochs\n",
    "        \"\"\"\n",
    "        x0, x1 = self.get_tensor_data()\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # set up optimizer\n",
    "        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n",
    "        nbatches = len(x0) // batch_size\n",
    "\n",
    "        # Start training (full batch)\n",
    "        for epoch in range(self.nepochs):\n",
    "            for j in range(nbatches):\n",
    "                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n",
    "                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n",
    "            \n",
    "            # probe\n",
    "            p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n",
    "\n",
    "            # get the corresponding loss\n",
    "            loss = self.get_loss(p0, p1)\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def repeated_train(self):\n",
    "        best_loss = np.inf\n",
    "        for train_num in range(self.ntries):\n",
    "            self.initialize_probe()\n",
    "            loss = self.train()\n",
    "            if loss < best_loss:\n",
    "                self.best_probe = copy.deepcopy(self.probe)\n",
    "                best_loss = loss\n",
    "\n",
    "        return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CCS without any labels\n",
    "ccs = CCS(neg_hs_train, pos_hs_train)\n",
    "ccs.repeated_train()\n",
    "\n",
    "# Evaluate\n",
    "ccs_acc = ccs.get_acc(neg_hs_test, pos_hs_test, y_test)\n",
    "print(\"CCS accuracy: {}\".format(ccs_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "823d24656dbc32b8dace34fa6a3a6806739c01ed161eb6681700e5f71dfa66e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
