{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import dataclasses\n",
    "import einops\n",
    "from CCS import CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "tqamc = load_dataset(\"truthful_qa\", \"multiple_choice\")['validation']\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"boolq\", split='train')\n",
    "# print(dataset[4])\n",
    "\n",
    "# def boolq_to_prompt(data, FLAG=1): \n",
    "#   if FLAG == 1: \n",
    "#     return \"Passage: \" + data['passage'] + \"\\n\\nAfter reading this passage, I have a question: \" + data['question'] + \"? Yes or no?\"\n",
    "\n",
    "# prompt = boolq_to_prompt(dataset[0])\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "config = GPT2Config.from_pretrained(\"gpt2-medium\", output_hidden_states=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_length = 40, do_sample=True, top_p = 0.95, top_k =60, **model_kwargs): \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    output_sequences = model.generate(inputs,max_length=max_length, \n",
    "                                      do_sample=do_sample,top_p=top_p,\n",
    "                                      top_k=top_k,**model_kwargs)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "out = generate(tqamc['question'][5] + \" \" + tqamc['mc2_targets'][5]['choices'][3])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT = 0.47869747411559416\n",
    "# inline with the truthfulQA paper result\n",
    "\n",
    "# corrects = []\n",
    "# for i in tqdm(range(len(tqamc['validation']))):\n",
    "#   prompt = tqamc['validation'][i]['question'] + \" \"\n",
    "#   mc2_dict = tqamc['validation'][i]['mc2_targets']\n",
    "#   corrects.append(normalized_correct_answers(prompt, mc2_dict))\n",
    "\n",
    "# print(sum(corrects)/len(corrects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tqa_mc2(tqamc, frac, start=True): \n",
    "    \"\"\"\n",
    "    TruthfulQA MC2 has multiple correct answers per question. Want to split into\n",
    "    multiple copies of the question with different answers so there's only two\n",
    "    options per datapoint\n",
    "    \"\"\"\n",
    "    dataset = {'question': [], 'choices': [], 'labels': []}\n",
    "    dlen = int(frac*len(tqamc['question']))\n",
    "    for i in tqdm(range(dlen)): \n",
    "        ncopies = len(tqamc['mc2_targets'][i]['labels'])//2\n",
    "        for l in range(ncopies): \n",
    "            dataset['question'].append(tqamc['question'][i])\n",
    "            dataset['choices'].append([tqamc['mc2_targets'][i]['choices'][l], \n",
    "                                      tqamc['mc2_targets'][i]['choices'][l+ncopies]])\n",
    "            dataset['labels'].append([tqamc['mc2_targets'][i]['labels'][l], \n",
    "                                      tqamc['mc2_targets'][i]['labels'][l+ncopies]])\n",
    "    return dataset\n",
    "\n",
    "dataset = split_tqa_mc2(tqamc, 0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'question': dataset['question'][100:], 'choices': dataset['choices'][100:],\n",
    "        'labels': dataset['labels'][100:]}\n",
    "train = {'question': dataset['question'][:100], 'choices': dataset['choices'][:100],\n",
    "         'labels': dataset['labels'][:100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, tokenizer, dataset, layers, frac): \n",
    "    \"\"\"\n",
    "    Takes in truthfulQA MC2 dataset (after split from above) and returns a \n",
    "    (len_dataset, 2, len_activations_at_layer * len(layers)) tensor and a \n",
    "    (len_dataset, 1) tensor of labels\n",
    "    \"\"\"\n",
    "    dlen = int(frac*len(dataset['question']))\n",
    "    inputs = [[tokenizer(text, return_tensors=\"pt\")[\"input_ids\"] for text in \n",
    "                 [dataset['question'][i] + \" \" + dataset['choices'][i][0],  \n",
    "                 dataset['question'][i] + \" \" + dataset['choices'][i][1]]] \n",
    "                 for i in range(dlen)]\n",
    "    # get activations at each layer\n",
    "    activations = t.stack([t.stack([t.stack([model(i)[2][layer][0,-1]for layer in layers]) for i in input]) for input in tqdm(inputs)])\n",
    "    activations = einops.rearrange(activations, 'dlen nchoices len_layers len_activations -> dlen nchoices (len_layers len_activations)')\n",
    "    labels = t.zeros(dlen, 1)\n",
    "    return t.tensor(activations), labels\n",
    "\n",
    "def get_one_activation(model, tokenizer, prompt, layer): \n",
    "    \"\"\"\n",
    "    Outputs activations at hidden_layer layer of model on the last token of prompt\n",
    "    \"\"\"\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return model(input)[2][layer][0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations, y = get_activations(model, tokenizer, train, [6], 1)\n",
    "# concat on dim 2\n",
    "print(\"\")\n",
    "print(activations.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CCS probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = activations[:,0,:].detach()\n",
    "x0 = (x0 - x0.mean(axis=0, keepdims=True))/x0.std(axis=0, keepdims=True)\n",
    "x1 = activations[:,1,:].detach()\n",
    "x1 = (x1 - x1.mean(axis=0, keepdims=True))/x1.std(axis=0, keepdims=True)\n",
    "\n",
    "ccs = CCS(x0, x1, y)\n",
    "print(\"\\n\", ccs.train())\n",
    "print(ccs.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test clasifier on test set\n",
    "test_acts, y_test = get_activations(model, tokenizer, test, [6], 1)\n",
    "# normalize with training set mean and std\n",
    "xtest0 = test_acts[:,0,:].detach()\n",
    "xtest0 = (xtest0 - x0.mean(axis=0, keepdims=True))/x0.std(axis=0, keepdims=True)\n",
    "xtest1 = test_acts[:,1,:].detach()\n",
    "xtest1 = (xtest1 - x1.mean(axis=0, keepdims=True))/x1.std(axis=0, keepdims=True)\n",
    "print(\"\\n\", ccs.pred_acc(xtest0, xtest1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_CCS_pred(data_index, ccs): \n",
    "    \"\"\"\n",
    "    View CCS predictions for both answer choices for dataset[data_index]\n",
    "    \"\"\"\n",
    "    q = train['question'][data_index] + \" \"\n",
    "    prmpt1 = q + train['choices'][data_index][0]\n",
    "    prmpt2 = q + train['choices'][data_index][1]\n",
    "    x0 = get_one_activation(model, tokenizer, prmpt1, 8).unsqueeze(0).detach()\n",
    "    x1 = get_one_activation(model, tokenizer, prmpt2, 8).unsqueeze(0).detach()\n",
    "    y = t.Tensor([0])\n",
    "    print(prmpt1)\n",
    "    print(prmpt2)\n",
    "    print(ccs.probe(x0))\n",
    "    print(ccs.probe(x1))\n",
    "    print(ccs.pred_acc(x0, x1, y))\n",
    "\n",
    "for i in range(50): \n",
    "    view_CCS_pred(i, ccs)\n",
    "\n",
    "# current weird things\n",
    "# 1) ties get resolved to successful predictions lol FIXED\n",
    "# 2) weird incentive to be (1,1) or (0,0)\n",
    "# 3) the accuracy for pred_acc is opposite for some reason FIXED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "acts = t.cat((x0, x1), dim=0)\n",
    "pca.fit(acts)\n",
    "\n",
    "x0_pca = pca.transform(x0)\n",
    "x1_pca = pca.transform(x1)\n",
    "\n",
    "plt.scatter(x0_pca[:,0], x0_pca[:,1], c='r')\n",
    "plt.scatter(x1_pca[:,0], x1_pca[:,1], c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test zero-shot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT:  0.46790703848183607\n",
    "# inline with the truthfulQA paper\n",
    "\n",
    "# test zero shot performance (by comparing relative logit value)\n",
    "def get_ll(prompt, text): \n",
    "    input_ids = tokenizer.encode(prompt + text, return_tensors='pt')\n",
    "    logits = model(input_ids)[0]\n",
    "    # get ll of last token\n",
    "    return logits[0, -1, tokenizer.encode(text)[-1]].item()\n",
    "\n",
    "def normalized_prob(prompt, choices): \n",
    "    lls = np.array([get_ll(prompt, choice) for choice in choices])\n",
    "    # turn into normalized probabilities that sum to 1\n",
    "    probs = np.exp(lls - lls.max())\n",
    "    return probs / probs.sum()\n",
    "\n",
    "dlen = len(test['question'])\n",
    "probs = []\n",
    "for i in tqdm(range(dlen)):\n",
    "    prompt = test['question'][i] + \" \"\n",
    "    choices = test['choices'][i]\n",
    "    probs.append(normalized_prob(prompt, choices)[0])\n",
    "\n",
    "print(\"\\n\",sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments to run\n",
    "## fine-tuning on TruthfulQA with CCS head output\n",
    "## hold the first model frozen and only finetune a second model\n",
    "## creating a wrapper model to get better performance on truthfulQA if fine-tuning fails (basically whats happening now)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
