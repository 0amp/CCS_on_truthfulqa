{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dataclasses\n",
    "from CCS import CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset truthful_qa (/Users/oamie/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/70210b72382652635215516e59663843b88eda16bd2acef909fb46700beb039a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97c308e2666426c810150ff5ee08632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tqamc = load_dataset(\"truthful_qa\", \"multiple_choice\")['validation']\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"boolq\", split='train')\n",
    "# print(dataset[4])\n",
    "\n",
    "# def boolq_to_prompt(data, FLAG=1): \n",
    "#   if FLAG == 1: \n",
    "#     return \"Passage: \" + data['passage'] + \"\\n\\nAfter reading this passage, I have a question: \" + data['question'] + \"? Yes or no?\"\n",
    "\n",
    "# prompt = boolq_to_prompt(dataset[0])\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "config = GPT2Config.from_pretrained(\"gpt2-medium\", output_hidden_states=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a distinction between left-brained and right-brained thinkers? No, there is no distinction between left-brained and right-brained thinkers. (And if there were,\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, max_length = 40, do_sample=True, top_p = 0.95, top_k =60, **model_kwargs): \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    output_sequences = model.generate(inputs,max_length=max_length, \n",
    "                                      do_sample=do_sample,top_p=top_p,\n",
    "                                      top_k=top_k,**model_kwargs)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "out = generate(tqamc['question'][5] + \" \" + tqamc['mc2_targets'][5]['choices'][3])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT = 0.47869747411559416\n",
    "# inline with the truthfulQA paper result\n",
    "\n",
    "# corrects = []\n",
    "# for i in tqdm(range(len(tqamc['validation']))):\n",
    "#   prompt = tqamc['validation'][i]['question'] + \" \"\n",
    "#   mc2_dict = tqamc['validation'][i]['mc2_targets']\n",
    "#   corrects.append(normalized_correct_answers(prompt, mc2_dict))\n",
    "\n",
    "# print(sum(corrects)/len(corrects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:06<00:00,  9.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def split_tqa_mc2(tqamc, frac, start=True): \n",
    "    \"\"\"\n",
    "    TruthfulQA MC2 has multiple correct answers per question. Want to split into\n",
    "    multiple copies of the question with different answers so there's only two\n",
    "    options per datapoint\n",
    "    \"\"\"\n",
    "    dataset = {'question': [], 'choices': [], 'labels': []}\n",
    "    dlen = int(frac*len(tqamc['question']))\n",
    "    for i in tqdm(range(dlen)): \n",
    "        ncopies = len(tqamc['mc2_targets'][i]['labels'])//2\n",
    "        for l in range(ncopies): \n",
    "            dataset['question'].append(tqamc['question'][i])\n",
    "            dataset['choices'].append([tqamc['mc2_targets'][i]['choices'][l], \n",
    "                                      tqamc['mc2_targets'][i]['choices'][l+ncopies]])\n",
    "            dataset['labels'].append([tqamc['mc2_targets'][i]['labels'][l], \n",
    "                                      tqamc['mc2_targets'][i]['labels'][l+ncopies]])\n",
    "    return dataset\n",
    "\n",
    "dataset = split_tqa_mc2(tqamc, 0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'question': dataset['question'][100:], 'choices': dataset['choices'][100:],\n",
    "        'labels': dataset['labels'][100:]}\n",
    "train = {'question': dataset['question'][:100], 'choices': dataset['choices'][:100],\n",
    "         'labels': dataset['labels'][:100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, tokenizer, dataset, layer, frac): \n",
    "    \"\"\"\n",
    "    Takes in truthfulQA MC2 dataset (after split from above) and returns a \n",
    "    (len_dataset, 2, len_activations_at_layer) tensor and a (len_dataset, 1)\n",
    "    tensor of labels\n",
    "    \"\"\"\n",
    "    dlen = int(frac*len(dataset['question']))\n",
    "    inputs = [[tokenizer(text, return_tensors=\"pt\")[\"input_ids\"] for text in \n",
    "                 [dataset['question'][i] + \" \" + dataset['choices'][i][0],  \n",
    "                 dataset['question'][i] + \" \" + dataset['choices'][i][1]]] for i in range(dlen)]\n",
    "    activations = t.stack([t.stack([model(i)[2][layer][0,-1] for i in input]) for input in tqdm(inputs)])\n",
    "    return activations, t.zeros(dlen)\n",
    "\n",
    "def get_one_activation(model, tokenizer, prompt, layer): \n",
    "    \"\"\"\n",
    "    Outputs activations at hidden_layer layer of model on the last token of prompt\n",
    "    \"\"\"\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return model(input)[2][layer][0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([100, 2, 1024])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "activations, y = get_activations(model, tokenizer, train, 8, 1)\n",
    "print(\"\")\n",
    "print(activations.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (0.5800000131130219, 1.3500005006790161)\n",
      "acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x0 = activations[:,0,:].detach()\n",
    "x0 = (x0 - x0.mean(axis=0, keepdims=True))/x0.std(axis=0, keepdims=True)\n",
    "x1 = activations[:,1,:].detach()\n",
    "x1 = (x1 - x1.mean(axis=0, keepdims=True))/x1.std(axis=0, keepdims=True)\n",
    "\n",
    "ccs = CCS(x0, x1, y)\n",
    "print(\"\\n\", ccs.train())\n",
    "print(ccs.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 120/120 [00:33<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5583333373069763\n"
     ]
    }
   ],
   "source": [
    "# test clasifier on test set\n",
    "test_acts, y_test = get_activations(model, tokenizer, test, 8, 1)\n",
    "# normalize with training set mean and std\n",
    "xtest0 = test_acts[:,0,:].detach()\n",
    "xtest0 = (xtest0 - x0.mean(axis=0, keepdims=True))/x0.std(axis=0, keepdims=True)\n",
    "xtest1 = test_acts[:,1,:].detach()\n",
    "xtest1 = (xtest1 - x1.mean(axis=0, keepdims=True))/x1.std(axis=0, keepdims=True)\n",
    "print(\"\\n\", ccs.pred_acc(xtest0, xtest1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def view_CCS_pred(data_index, ccs): \n",
    "    \"\"\"\n",
    "    View CCS predictions for both answer choices for dataset[data_index]\n",
    "    \"\"\"\n",
    "    q = train['question'][data_index] + \" \"\n",
    "    prmpt1 = q + train['choices'][data_index][0]\n",
    "    prmpt2 = q + train['choices'][data_index][1]\n",
    "    x0 = get_one_activation(model, tokenizer, prmpt1, 8).unsqueeze(0).detach()\n",
    "    x1 = get_one_activation(model, tokenizer, prmpt2, 8).unsqueeze(0).detach()\n",
    "    y = t.Tensor([0])\n",
    "    print(prmpt1)\n",
    "    print(prmpt2)\n",
    "    print(ccs.probe(x0))\n",
    "    print(ccs.probe(x1))\n",
    "    print(ccs.pred_acc(x0, x1, y))\n",
    "\n",
    "for i in range(50): \n",
    "    view_CCS_pred(i, ccs)\n",
    "\n",
    "# current weird things\n",
    "# 1) ties get resolved to successful predictions lol FIXED\n",
    "# 2) weird incentive to be (1,1) or (0,0)\n",
    "# 3) the accuracy for pred_acc is opposite for some reason FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5799999833106995\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(ccs.pred_acc(x0, x1, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 605/605 [02:38<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.46790703848183607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RESULT:  0.46790703848183607\n",
    "# inline with the truthfulQA paper\n",
    "\n",
    "# test zero shot performance (by comparing relative logit value)\n",
    "def get_ll(prompt, text): \n",
    "    input_ids = tokenizer.encode(prompt + text, return_tensors='pt')\n",
    "    logits = model(input_ids)[0]\n",
    "    # get ll of last token\n",
    "    return logits[0, -1, tokenizer.encode(text)[-1]].item()\n",
    "\n",
    "def normalized_prob(prompt, choices): \n",
    "    lls = np.array([get_ll(prompt, choice) for choice in choices])\n",
    "    # turn into normalized probabilities that sum to 1\n",
    "    probs = np.exp(lls - lls.max())\n",
    "    return probs / probs.sum()\n",
    "\n",
    "dlen = len(test['question'])\n",
    "probs = []\n",
    "for i in tqdm(range(dlen)):\n",
    "    prompt = test['question'][i] + \" \"\n",
    "    choices = test['choices'][i]\n",
    "    probs.append(normalized_prob(prompt, choices)[0])\n",
    "\n",
    "print(\"\\n\",sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments to run\n",
    "## fine-tuning on TruthfulQA with CCS head output\n",
    "## hold the first model frozen and only finetune a second model\n",
    "## creating a wrapper model to get better performance on truthfulQA if fine-tuning fails (basically whats happening now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
